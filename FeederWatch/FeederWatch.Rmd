---
title: "R Notebook - Bird FeederWatch dataset"
---

```{r}
libs <- c("tidyverse","dplyr", "maps", "Amelia", "ggrepe", "usmap", "RColorBrewer","tidymodels", "vip","caret","rpart.plot")

installed_libs <- libs %in% rownames(installed.packages())

if(any(installed_libs == F)) {
  install.packages(libs[!installed_libs])
} else{
  print("All the libraries already installed")
}
```

```{r}
library(tidyverse)
library(dplyr)
library(maps)
library(Amelia)
library(ggrepel)
library(usmap)
library(RColorBrewer)
library(tidymodels)
library(vip)
library(caret)
library(rpart)
library(rpart.plot)
```

The data comes from the Project FeederWatch.

```{r}
feederwatch <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_2021_public.csv')
site_data <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv')
```

## Quick peek on data

```{r}
head(feederwatch)
head(site_data)

str(feederwatch)

glimpse(feederwatch)
```

## Data preparation -\> feederwatch

I dropped 5 columns that I decided weren't provided significant information. Then I explored how much missing data I have in that dataset. I genereted plot showing where missingness occurs in the dataset. I found that in that dataset I had only 1% of missing data NA - in 1 column which was snow_dep_atleast. I decided to replace NA value with calculated median value for each region.

```{r}
feederwatch <- feederwatch %>% 
  select(-entry_technique,-PROJ_PERIOD_ID,-Data_Entry_Method,-sub_id,-obs_id)

feederwatch %>% 
  missmap(
    main = "FeederWatch - Missing Map",
    col = c("red","black")
  )

feederwatch %>% 
  select(snow_dep_atleast) %>% 
  table(useNA = "ifany")

feederwatch %>% 
  select(subnational1_code) %>% 
  table()

(median_by_region <- feederwatch %>%
  group_by(subnational1_code) %>%
  summarize(median_value = median(snow_dep_atleast, na.rm = TRUE)))

feederwatch <- feederwatch %>% 
  mutate(snow_dep_atleast = ifelse(is.na(snow_dep_atleast) & subnational1_code %in% median_by_region$subnational1_code,median_by_region$median_value[match(subnational1_code, median_by_region$subnational1_code)],snow_dep_atleast)) %>% 
  mutate(Date = as.Date(paste(Year,Month,Day,sep = "-"))) %>% 
  select(-Year,-Month,-Day)


feederwatch %>% 
  select(snow_dep_atleast) %>% 
  table(useNA = "ifany")
```

At chunk below I've prepared data to generation plot. I've used `usmap` package in combination with `ggplot2` to generate the US map. On the generated plot, I've intended to show the depth of the snow cover depending on the state.

```{r}
(region_dataset_USA <- feederwatch %>% 
  select(subnational1_code,snow_dep_atleast) %>% 
  filter(grepl("US", subnational1_code)) %>% 
  mutate(state_abb = substring(subnational1_code, 4)) %>%
  select(-subnational1_code) %>% 
  mutate(state = state.name[match(state_abb, state.abb)]) %>%
  select(-state_abb) %>% 
  group_by(state) %>% 
  summarise(mean_snow_dep = mean(snow_dep_atleast)))

states <- c(state.name, "District of Columbia")
states 

missing_states <- setdiff(states, region_dataset_USA$state)

region_dataset_USA <- region_dataset_USA %>% 
  mutate(state = ifelse(is.na(state),"District of Columbia",state))

genereting_of_snow_depth_plot_USA <- function(input_data,year) {
  plot_usmap(data = input_data, values = "mean_snow_dep", color = "grey") +
  scale_fill_gradient(low = "white", high = "violet", na.value = "gray", guide = "legend") +
  labs(title = "Median Snow Depth by Region",
       subtitle = paste("in the USA states for year",year)) +
  theme(legend.position = "right")
}
  
snow_depth_plot <- genereting_of_snow_depth_plot_USA(region_dataset_USA,2021)

snow_depth_plot

```

In this case I could have replaced the NA value in dataset `region_dataset_USA` as it was the only one missing a value. Knowing that there are 50 states (+ 1 federal district) in the USA I could have compared and easily found the missing state - this would be impossible in case there were 2 or more missing values.

Like we could expect value of mean minimum snow depth which had been estimated by participant, was the highest in Alaska. This is the most northerly state in the USA so it is logical assumption, which is confirmed by above plot. In the southern states value of mean minimum snow depth is close to the zero, and increase with latitude. This subset of dataset comes from year 2021. So I have been decided to compare date from other years to check the difference in snowfall between these years\*.

```{r}
feederwatch_1988_1995 <- readr::read_csv('https://clo-pfw-prod.s3-us-west-2.amazonaws.com/data/PFW_1988_1995_public.csv')

head(feederwatch_1988_1995)

feederwatch_1988_1995 %>% 
  filter(Year == 1989) %>% 
  select(SNOW_DEP_ATLEAST) %>% 
  table(useNA = "ifany")

median_by_region_1989 <- feederwatch_1988_1995 %>%
  filter(Year == 1989) %>% 
  group_by(SUBNATIONAL1_CODE) %>%
  summarize(median_value = median(SNOW_DEP_ATLEAST, na.rm = TRUE))

feederwatch_1989 <- feederwatch_1988_1995 %>% 
  filter(Year == 1989) %>% 
  select(SNOW_DEP_ATLEAST,SUBNATIONAL1_CODE,Year) %>% 
  mutate(SNOW_DEP_ATLEAST = ifelse(is.na(SNOW_DEP_ATLEAST) & SUBNATIONAL1_CODE %in% median_by_region_1989$SUBNATIONAL1_CODE,median_by_region_1989$median_value[match(SUBNATIONAL1_CODE,median_by_region_1989$SUBNATIONAL1_CODE)],SNOW_DEP_ATLEAST)) 

region_dataset_USA_1989 <- feederwatch_1989 %>% 
  select(SUBNATIONAL1_CODE,SNOW_DEP_ATLEAST) %>% 
  filter(grepl("US", SUBNATIONAL1_CODE)) %>% 
  mutate(state_abb = substring(SUBNATIONAL1_CODE,4)) %>% 
  mutate(state = state.name[match(state_abb,state.abb)]) %>% 
  select(-SUBNATIONAL1_CODE,-state_abb) %>% 
  group_by(state) %>% 
  summarise(mean_snow_dep = mean(SNOW_DEP_ATLEAST))
  
missing_states <- setdiff(states,region_dataset_USA_1989$state)

region_dataset_USA_1988 <- region_dataset_USA_1988 %>% 
  mutate(state = ifelse(is.na(state),"Hawaii",state))
```

\*Data available through 1988 is available for download on FeederWatch Raw Dataset Downloads page.

I've decided to replace NA value in dataset for 1988 with median for group like I did for the primary dataset. I've started with the data for year 1988 but there was a problem - I couldn't calculate median because in column `SNOW_DEP_ATLEAST` for 1988 were only NA! It has occurred to me that in 1988 participants didn't have to estimate snow cover. It was better for 1989 because it was only 280611 records with NA value for snow depth.

I've cleaned the data in the same way I've cleaned the primary dataset. During that action I found difference between these datasets - primary dataset includes 51 states while the second dataset includes only 50 states. The reason for the difference stems from the fact that the first dataset includes federal district of Columbia while the second doesn't have it. Variable missing_states in this case is character vector with 2 elements - "Hawaii" and "District of Columbia". Previously I've replaced the NA value with District of Columbia but in this case I have decided to replace it Hawaii.

For the purpose of comparison, I've decided to delete the row with federal District of Columbia in dataset `region_dataset_USA` and after that, compare results based on 50 states.

```{r}
region_dataset_USA <- subset(region_dataset_USA,region_dataset_USA$state != "District of Columbia")

snow_depth_plot_to_compare_1989 <- genereting_of_snow_depth_plot_USA(region_dataset_USA_1988,1989)
snow_depth_plot_to_compare_2021 <- genereting_of_snow_depth_plot_USA(region_dataset_USA,2021)

snow_depth_plot_to_compare_1989
snow_depth_plot_to_compare_2021
```

What is surprising, according to the generated plot in 1989 there were much more states with mean of snow depth equal 0 than in 2021. On the other hand, the highest value belongs to Alaska in both case but in 1989 this value was higher than in 2021.

The interpretation of obtained result:

1.  Although FeederWatch creator's attempt to minimize errors, a small percentage of FeederWatch reports are incorrect and analysts must be aware that misidentifications, data entry errors, and other sources of error can evade their data validation system. It's a logical assumption that in their early years of operating errors were more common. Over time, every organization tends to evolve and improve its systems and processes based on lessons learned from past errors or inefficiencies.

2.  Minimum snow depth was estimated by participants. That can be the source of discrepancy with obtained results because participants may have different interpretations of what constitutes as minimum depth, leading to inconsistencies in their estimates. They might also used different methods or tools to measure snow depth. And last but not least, for the first years errors might be caused by lack of standardization. Without clear guidelines or instructions on how to measure snow depth, participants might have different understanding of what measurement should be considered as the minimum. The absence of standardized procedures can result in inaccurate estimates.

```{r}
snow_dept_diffrences_between_years_1989_2021 <- region_dataset_USA_1989 %>% 
  mutate(diff = ifelse(region_dataset_USA$state %in% state,(mean_snow_dep - region_dataset_USA$mean_snow_dep),NA)) %>% 
  select(state,diff)

snow_dept_diffrences_between_years_1989_2021 %>% filter(diff > 0)
snow_dept_diffrences_between_years_1989_2021 %>% filter(diff <= 0)
```

I've decided to compare result in tibble to see the exact difference between these years. In 14 states the difference was greater than 0 which means that in 1989 value of mean snow depth was higher than in 2021. For the rest - the value of mean snow depth was higher in 2021 than in 1989.

```{r}
head(site_data)
str(site_data)
glimpse(site_data)
```

## Data preparation - side_data

For starters, I've decided to generete missing plot. In this dataset I've found 26% of missingness! Most of them are in columns describing numfeeders but there are missingness everywhere except loc_id. I divided this dataset into smaller with specific information. In dataset `animals_activity_data` there is only 8% of missingness. I've changed NA on 0 in animals activity set.

```{r}
site_data %>% 
  missmap(
    main = "Site data - Missing Map",
    col = c("red","black")
  )

site_data <- site_data %>% 
  mutate(year = str_extract(proj_period_id, "\\d+"))

str(site_data$year)
head(site_data)

months_years_data <- site_data %>% 
  select(loc_id,year,fed_in_jan,fed_in_feb,fed_in_mar,fed_in_apr,fed_in_may,fed_in_jun,fed_in_jul,fed_in_aug,fed_in_sep,fed_in_oct,fed_in_nov,fed_in_dec)
  
head(months_years_data)

months_years_data %>% 
  missmap(
    main = "Months by year - Missing Map",
    col = c("orange","black")
  )

animals_activity_data <- site_data %>% 
  select(loc_id,year,nearby_feeders,squirrels,cats,dogs,humans)

animals_activity_data %>% 
  missmap(
    main = "Animal activity - Missing Map",
    col = c("yellow","black")
  )

head(animals_activity_data)

animals_activity_data %>% 
  select(squirrels) %>% 
  table(useNA = 'ifany')

site_data <- site_data %>% 
  mutate(nearby_feeders = replace_na(nearby_feeders, 0),
         squirrels = replace_na(squirrels, 0),
         cats = replace_na(cats, 0),
         dogs = replace_na(dogs, 0),
         humans = replace_na(humans, 0)) 

animals_activity_data <- animals_activity_data %>% 
  mutate(nearby_feeders = replace_na(nearby_feeders, 0),
         squirrels = replace_na(squirrels, 0),
         cats = replace_na(cats, 0),
         dogs = replace_na(dogs, 0),
         humans = replace_na(humans, 0)) 

animals_activity_data %>% 
  missmap(
    main = "Animal activity - Missing Map",
    col = c("yellow","black")
  )

animals_activity_data %>% 
  select(humans) %>% 
  table(useNA = 'ifany')

animals_activity_data %>%
  group_by(squirrels) %>%
  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))

animals_activity_data %>%
  group_by(cats) %>%
  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))

animals_activity_data %>%
  group_by(dogs) %>%
  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))

animals_activity_data %>%
  group_by(humans) %>%
  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))

site_data <- site_data %>% 
  mutate(Year = str_extract(proj_period_id, "\\d+"))
```

I have a dataset named `animals_activity_data` which contains information about animals and nearby feeders - 1 means presence and 0 means absence of specific animal. Based of obtained result, it appears that presence of squirrels is more common than their absence. Similarly, presence of cats, dogs and human is more common than their absence.

The code and results only provide information about the average number of nearby feeders for different groups of squirrels, cats, dogs and humans. It doesn't provide information about the frequency or distribution of the presence or absence of these animals in the dataset. So I decided to look up for these parameters.

```{r}
squirrel_counts <- table(animals_activity_data$squirrels)
cat_counts <- table(animals_activity_data$cats)

animals_activity_data <- animals_activity_data %>% 
  mutate(presence_squirrels = ifelse(squirrels == 1, "Presence", "Absence"),
         presence_cats = ifelse(cats == 1, "Presence", "Absence"),
         presence_dogs = ifelse(dogs == 1, "Presence", "Absence"),
         presence_humans = ifelse(humans == 1, "Presence", "Absence")) 

combined_data <- rbind(
  data.frame(animals = "Squirrels", presence = animals_activity_data$presence_squirrels),
  data.frame(animals = "Cats", presence = animals_activity_data$presence_cats),
  data.frame(animals = "Dogs", presence = animals_activity_data$presence_dogs),
  data.frame(animals = "Humans", presence = animals_activity_data$presence_humans)
)

my_palette <- brewer.pal(4, "Pastel1")

present_absent_barplot <- combined_data %>% 
  ggplot(aes(x = presence, fill = animals)) +
  geom_bar(position = "dodge", color = "black", stat = "count") +
  labs(title = "Presence or absence of animals in the dataset",
       x = "Presence or Absence", 
       y = "Count") +
  scale_fill_manual(values = my_palette)

# Count the occurrences of squirrels and cats in the dataset:
squirrel_count <- sum(animals_activity_data$squirrels == 1, na.rm = TRUE)
cat_count <- sum(animals_activity_data$cats == 1, na.rm = TRUE)
dog_count <- sum(animals_activity_data$dogs == 1, na.rm = TRUE)
human_count <- sum(animals_activity_data$humans == 1, na.rm = TRUE)

#Calculate the total number of records in the dataset:
total_records <- animals_activity_data %>% 
  nrow(.)

#Calculate the proportion of records where animals are present:
squirrel_presence_proportion <- squirrel_count / total_records
cat_presence_proportion <- cat_count / total_records
dog_presence_proportion <- dog_count / total_records
human_presence_proportion <- human_count / total_records

print(c(squirrel_presence_proportion, cat_presence_proportion, dog_presence_proportion, human_presence_proportion))
```

The interpretation of obtained result:

1.  The proportion of records where squirrels are present (0.7484107) is relatively high, indicating that squirrels are frequently observed in the dataset. This suggests that squirrels are the most common among the animals in the dataset.

2.  The proportion of records where cats are present (0.4731379) indicates that cats are present in less than half of the records. This suggest that presence of cats is relatively less common compered to presence of squirrels.

3.  The proportion of records where dogs are present (0.4629514) is relatively low - the lowest value. This suggest that dogs are the least observed compared to other animals

4.  The proportion of records where humans are present (0.7287649) is relatively high, indicating that humans are frequently observed in the dataset. This suggest that humans are a common presence among the animals, potentially indicating their involvement in observation or it can have connection with habitat with high urbanization.

At chunk below I've prepared data for plotting. I've intended to show how total of bird's population number changing through time in North America.

```{r}
species_counts <- feederwatch %>% 
  group_by(Week = format(Date,"%Y-%U"),species_code) %>% 
  summarise(total_population = sum(how_many)) %>% 
  arrange(desc(total_population))

#Check the total number of species in dataset
(species_counts %>% 
  group_by(species_code) %>% 
  summarise(total_population = sum(total_population)))
  
top_species <- species_counts %>% 
  group_by(species_code) %>%
  summarise(total_population = sum(total_population)) %>%
  top_n(5, total_population) %>% 
  ungroup()
  
species_counts <- species_counts %>% 
  filter(species_code %in% top_species$species_code)

species_counts$Week <- as.Date(paste0(substr(species_counts$Week, 1, 4), "-W", substr(species_counts$Week, 6, 7), "-1"), format = "%Y-W%U-%u")

species_labels <- c("daejun" = "Daejun magpie",
                    "houspa" = "House Sparrow",
                    "moudov" = "Mourning Dove",
                    "amegfi" = "American Goldfinch",
                    "houfin" = "House Finch ")

species_colors <- c("daejun" = "red",
                    "houspa" = "blue",
                    "moudov" = "green",
                    "amegfi" = "yellow",
                    "houfin" = "pink ")

generating_of_species_population_plot <- function(input_data,year,input_species_colors,input_species_labels) {
  input_data %>%
    ggplot(aes(x = Week, y = total_population, color = species_code, group = species_code)) +
    geom_line() +
    labs(title = "The 5 largest in number bird's population",
         subtitle = paste("Weekly observation from 13th November to 30th April ", year),
         x = "",
         y = "Total population") +
    scale_color_manual(values = input_species_colors, labels = input_species_labels) +
    scale_x_date(breaks = "2 weeks", date_labels = "%Y-%m-%d") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

species_counts_Nov_to_Apr_plot <- generating_of_species_population_plot(species_counts,2021,species_colors,species_labels)

species_counts_Nov_to_Apr_plot
```

I created line plot for 5 species of birds because dataset has data for 361 different species. It's a lot and any try to generate line plot for every species would make the line plot illegible. I've also decided that population calculation based on weekly observation would give the best result to show on line plot. I had to figure out breaks on x axis by trial and error and in my opinion 2 weeks breaks is the most readable.

```{r}
map_plot <- feederwatch %>% 
  ggplot(aes(x =longitude, y = latitude)) +
  borders("world") +
  geom_point(size = 0.5) +
  labs(title = "Map plot for observations", x = "Longitude", y = "Latitude")

map_plot
```

In the above excerpt I've created a simple map of the world, where based on latitude and longitude, observations are marked. Below I have focused on the map of North America, where it is clear that more sightings have been made in the eastern part of the continent.

```{r}
north_america_data <- feederwatch %>% 
  filter(grepl("US|CA", subnational1_code)) 

north_america_plot <- north_america_data %>% 
  ggplot(aes(x = longitude, y = latitude)) +
  borders("world",fill = "black", colour = "black",xlim = c(-300, -60), ylim = c(20, 50)) +
  geom_point(colour="red",size = 0.5) +
  labs(title = "North America Map plot for observations", 
       x = "Longitude", 
       y = "Latitude")

north_america_plot

(north_america_data %>% select(subnational1_code) %>% filter(grepl("CA",subnational1_code)) %>% nrow(.))/ (north_america_data %>% select(subnational1_code) %>% nrow(.))
```

Then, after checking the ratio of canadian observations to all records (it was approximately 18%) I've decided for focus only on american observations.

```{r}
usa_data <- feederwatch %>% 
  filter(grepl("US", subnational1_code) & !grepl("US-HI|US-AK", subnational1_code))

usa_plot <- usa_data %>% 
  ggplot(aes(x = longitude, y = latitude)) +
  borders("usa",fill = "black", colour = "black",xlim = c(-300, -60), ylim = c(20, 50)) +
  geom_point(colour="red",size = 0.5) +
  labs(title = "USA Map plot for observations",
       subtitle = "without Alaska and Hawaii",
       x = "Longitude", 
       y = "Latitude")

usa_plot
```

It's clear that the highest density of observation has been on north-eastern part of the USA.

```{r}
area_data_tree_shrubs <- site_data %>% 
  select(Year,evgr_trees_atleast, evgr_shrbs_atleast, dcid_trees_atleast, dcid_shrbs_atleast, fru_trees_atleast, cacti_atleast) %>% 
  group_by(Year)

area_data_tree_shrubs %>% 
  missmap(
    main = "Minimum number of trees or shrubs in the count area",
    col = c("orange","black")
  )

area_data_tree_shrubs <- area_data_tree_shrubs  %>% 
  filter_all(all_vars(!is.na(.)))
# filter(complete.cases(.))  <- dlaczego przestało działać?

area_data_tree_shrubs

sum_per_year <- area_data_tree_shrubs %>% 
  group_by(year) %>% 
  summarize(total_evergreen_trees = sum(evgr_trees_atleast),
            total_evergreen_shrubs = sum(evgr_shrbs_atleast),
            total_deciduous_trees = sum(dcid_trees_atleast),
            total_deciduous_shrubs = sum(dcid_shrbs_atleast),
            total_fruit_trees = sum(fru_trees_atleast),
            total_cacti = sum(cacti_atleast))
```

```{r}
plant_labels <- c("total_evergreen_trees" = "Evergreen trees",
                    "total_evergreen_shrubs" = "Evergreen shrubs",
                    "total_deciduous_trees" = "Deciduous trees",
                    "total_deciduous_shrubs" = "Deciduous shrubs",
                    "total_fruit_trees" = "Fruit trees",
                    "total_cacti" = "Cacti")

plant_colors <- c("total_evergreen_trees" = "hotpink1",
                  "total_evergreen_shrubs" = "mediumorchid1",
                  "total_deciduous_trees" = "lightsalmon",
                  "total_deciduous_shrubs" = "lightpink3",
                  "total_fruit_trees" = "lightpink4",
                  "total_cacti" = "palevioletred4")

pie_chart <- list() #why? wcześniej działało bez tego?

for (i in 1:nrow(sum_per_year)) {
  year_data <- sum_per_year[i, ]
  
  pie_data <- data.frame(
    category = names(year_data)[-1], 
    value = unlist(year_data[-1]))
  
  pie_chart[[i]] <- pie_data %>% 
    ggplot(aes(x = "", y = value, fill = category)) +
    geom_col(width = 1, color = 1) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar(theta = "y") +
    labs(x = "",
         y = "",
         title = "Total of minimum number of trees or shrubs in the count area",
         subtitle = paste("Pie chart for year", year_data$year),
         fill = "Type of trees or shrubs") +
    scale_fill_manual(values = plant_colors, labels = plant_labels) +
    theme_void()
}

pie_chart[1]
pie_chart[11]
pie_chart[28]
```

```{r}
habitat_data <- site_data %>%
  select(year,hab_dcid_woods,hab_evgr_woods,hab_mixed_woods,hab_orchard,hab_park,hab_water_fresh,hab_water_salt,hab_residential,hab_industrial,hab_agricultural,hab_desert_scrub,hab_young_woods,hab_swamp,hab_marsh)

habitat_data %>% 
  missmap(
    main = "Habitat area - Missing Map",
    col = c("green","black")
  )

habitat_data <- habitat_data %>% 
  filter(complete.cases(.))

sum_hab_per_year <- habitat_data %>% 
  group_by(year) %>% 
  summarize(total_hab_dcid_woods = sum(hab_dcid_woods),
            total_hab_evgr_woods = sum(hab_evgr_woods),
            total_hab_mixed_woods = sum(hab_mixed_woods),
            total_hab_orchard = sum(hab_orchard),
            total_hab_park = sum(hab_park),
            total_hab_water_fresh = sum(hab_water_fresh),
            total_hab_water_salt = sum(hab_water_salt),
            total_hab_residential = sum(hab_residential),
            total_hab_industrial = sum(hab_industrial),
            total_hab_agricultural = sum(hab_agricultural),
            total_hab_desert_scrub = sum(hab_desert_scrub),
            total_hab_young_woods = sum(hab_young_woods),
            total_hab_swamp = sum(hab_swamp),
            total_hab_marsh = sum(hab_marsh))
```

```{r}
habitat_labels <- c("hab_dcid_woods" = "decidious woods",
                    "hab_evgr_woods" = "evergreen woods",
                    "hab_mixed_woods" = "mixed woods",
                    "hab_orchard" = "orchard",
                    "hab_park" = "park",
                    "hab_water_fresh" = "fresh water",
                    "hab_water_salt" = "salt water",
                    "hab_residential" = "residential",
                    "hab_industrial" = "industrial",
                    "hab_agricultural" = "agricultural",
                    "hab_desert_scrub" = "desert scrub",
                    "hab_young_woods" = "young woods",
                    "hab_swamp" = "swamp",
                    "hab_marsh" = "marsh")

bar_plot <- list()

for(i in 1:nrow(sum_hab_per_year)) {
  
  year_data <- sum_hab_per_year[i, ]
  
  bar_data <- data.frame(
    habitat = names(year_data)[-1],
    value = unlist(year_data[-1])
  )
  
 bar_plot[[i]] <- bar_data %>% 
   ggplot(aes(x = habitat, y = value)) +
    geom_bar(stat = "identity", fill = "orchid3") + 
    labs(
      title = "Total of habitat type",
      subtitle = paste("Bar plot for year", year_data$year),
      x = "Habitat",
      y = "Total"
    ) +
   scale_x_discrete(labels = habitat_labels) + #jak zrobic custom labels?????? bo poprzednio działa a tu nie
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
   
}

bar_plot[1]
bar_plot[15]
bar_plot[22]
```

```{r}
selected_species_code <- "pingro"#readline(prompt = "For what species do you want see plot?")

species_data <- feederwatch %>% 
  filter(species_code == selected_species_code) %>% 
  filter(grepl("US", subnational1_code) & !grepl("US-HI|US-AK", subnational1_code)) %>% 
  select(species_code,subnational1_code,how_many) %>% 
  group_by(subnational1_code) %>% 
  summarize(total_observation = sum(how_many))

values <- pull(species_data, total_observation)

state_density_plot <- plot_usmap(values = values, color = "orange", labels=FALSE) +
  scale_fill_continuous( low = "white", high = "orange", 
                         name = "Popularity") + 
  labs(title = "U.S. States",
       subtitle = paste("This is a density of observation for", selected_species_code)) + 
  theme(legend.position = "right") + 
  theme(panel.background = element_rect(colour = "black")) #nie rozumiem czemu nie działa????

state_density_plot
```

At chunk below, I've prepared scatter plot which aims to show the distribution and variation of bird observations over time for different bird species, allowing for comparisons and insights into the patterns and trends of bird sightings.

```{r}
bird_data <- feederwatch %>% 
  select(species_code,how_many,day1_am,day1_pm,day2_am,day2_pm,Date) %>% 
  filter(species_code == 'lessca' | species_code == 'cedwax' | species_code == 'yerwar')

bird_data$Date <- as.POSIXct(bird_data$Date)

scatter_plot <- bird_data %>% 
  ggplot(aes(x = Date, y = how_many, color = species_code)) +
  geom_point() +
  labs(x = "Datetime - month", y = "Number of bird's for one observation ") +
  theme_minimal()

scatter_plot

feederwatch %>%
    select(species_code) %>% 
  table(useNA = "ifany") %>%
  sort(decreasing = TRUE)

bird_data %>%
    select(species_code,how_many) %>% 
  table(useNA = "ifany")

median_for_species <- function(bird) {
  data <- bird_data[bird_data$species_code == bird, ]
  data$how_many <- as.numeric(data$how_many)
  return (median(data$how_many, na.rm = TRUE))
}

(cedwax_median <- median_for_species("cedwax"))
(yerwar_median <- median_for_species("yerwar"))
(lessca_median <- median_for_species("lessca"))

```

I've chosen 3 "different" kind of bird - `yerwar` (Yellow-rumped Warbler), `cedwax` (Cedar Waxwing ), `lessca` (Lesser Scaup), for purpose to show 3 different distribution. Based on generated plot I've had the following conclusions:

1.  The counts for Yellow-rumped Warbler remain consistent throughout the observed periods. The outlier for this species, which is significantly different from other observations, is 20 but still relatively low compared to other species. Yellow-rumped Warbler are frequently observed - this is one of the most common species in dataset `feederwatch`.

2.  The counts for Cedar Waxwings vary across the observed periods. The highest count for this species is 250, observed during one of the days. Median for number of birds per observation from over 100 record is 8. This indicates that there is a wide range of counts for Cedar Waxwings, with some observations reaching as high as 250. However, the median count of 8 suggests that the majority of the counts fall below this maximum value. Number of record indicates that Cedar Waxwings are still relatively common and frequently seen in the recorded data.

3.  There is only one record for Lesser Scaup. Since there is only one observation, the median represents the count value itself. Thus, 500 individuals for the single record indicates that its typically live in herds and are considered rare. However, it's important to note that the behavior and population status of species can vary depending on various factors such as habitat, geographic location, and conservation status.

# Modeling

## Random Forests

I've worked with the `feederwatch_df` dataframe in this modeling. A row in this data frame represents one observation. Each observation has included i.e., species.

### Preprocessing and preparation

Before using Random Forest with the FeederWatch dataset, I've needed to preprocess and prepare the data. This involved:

1.  Handling missing values, which I've done earlier in this notebook.

```{r}
feederwatch %>% 
  missmap(
    main = "FeederWatch - Missing Map",
    col = c("red","black")
  )
```

2.  Encoding categorical variables - feederwatch dataset has been included categorical features (e.g.,species_code), I've converted them into numerical representations, such as one-hot encoding using the `caret` package.

When dealing with a large number of categories (361 different species in this case), one-hot encoding can lead to a significant increase in the dimensionality of dataset. This increase in dimensionality can result in a sparse matrix and potentially pose challenges for model training and performance. Moreover, if some species have very few occurrences in the dataset, it may be difficult for the model to learn meaningful patterns from those rare categories.

In such case, I've decided to filter feederwatch dataset and to choose the most numerous species while the remaining species grouping into separate "other" category. This approach helps to reduce dimensionality and prevent overfitting caused by rare categories.

```{r}
feederwatch_df <- feederwatch %>% 
  as.data.frame(.)

top_species <- c("daejun", "houspa", 'moudov')
other_category <- "Other"

filtered_species <- feederwatch_df %>% 
  select(-loc_id, -latitude, -longitude,-subnational1_code,-Date) %>% 
  mutate(species = ifelse(species_code %in% top_species, species_code, other_category)) %>% 
  select(-species_code) %>% 
  as.data.frame(.)

# Perform one-hot encoding
dummy <- dummyVars(" ~ .", data=filtered_species)
encoding_feederwatch_df <- data.frame(predict(dummy, newdata = filtered_species)) 
```

I've used the `dummyVars()` function to create a model formula for one-hot encoding based on the filtered_species variable. Then, I've used the `predict()` function to perform the one-hot encoding on the new data, which is a data frame containing the filtered_species variable.

3.  Splitting the data - I've divided dataset into a training set and a testing set. I've used the training set to train the Random Forest model, while the testing set is used to evaluate its performance.

```{r}
# Set the random seed based on POSIX time
set.seed(as.numeric(Sys.time()))

encoding_feederwatch_df$valid <- as.factor(encoding_feederwatch_df$valid)

feederwatch_forest_split <- initial_split(encoding_feederwatch_df, prop = 0.75, strata = valid)

feederwatch_forest_training <- feederwatch_forest_split %>%
  training()

feederwatch_forest_test <- feederwatch_forest_split %>%
  testing()
```

???I've used "speciesOther" as response variable to predict the presence or absence of species other than the specified ones.

I've used "valid" as response variable to predict whether an observation is considered valid or not.

I've created folds for cross validation on the training data set

```{r}
set.seed(123)

feederwatch_forest_folds <- vfold_cv(feederwatch_forest_training, v = 5)
```

### Feature Engineering

I've prepared a feature engineering recipe for this data. I've trained the following transformations on training data. I've removed skewness from numeric predictors and normalized all numeric predictors.

```{r}
feederwatch_recipe <- recipe(
  valid ~ .,
  data = feederwatch_forest_training
  ) %>%
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_normalize(all_numeric(), -all_outcomes())

feederwatch_recipe %>% 
  prep() %>% 
  bake(new_data = feederwatch_forest_training)
```

### Model Specification

Next, I've specified a decision tree classifier with the following hyperparameters: - `cost_complexity`: The cost complexity parameter (a.k.a. Cp or λ) - `tree_depth`: The maximum depth of a tree - `min_n`: The minimum number of data points in a node that are required for the node to be split further.

I've used `decision_tree()` function to specify a a decision tree model with tidymodels. Hyperparameters can be adjusted to set specific values and customize the behavior of the decision tree model. However, if tuning is required, then each of these parameters must be set to `tune()`.

```{r}
tree_model <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
  ) %>%
  set_engine('rpart') %>% 
  set_mode('classification')
```

I've used the `rpart` engine because it provides opportunity with the ability to easily create visual plots of the decision trees using the `rpart.plot()` function. This helps to visualize and interpret the structure of the decision tree models more effectively.

### Workflow

After preparing the model and recipe, I've combined them into a workflow, which allows for easily management of the model-building process.

```{r}
tree_workflow <- workflow() %>%
  add_model(tree_model) %>% 
  add_recipe(feederwatch_recipe)
```

### Hyperparameter Tuning

To optimize the performance of decision tree models, I've conducted a grid search. This involves systematically trying out different combinations of hyperparameters for the decision tree algorithm. During this grid search process, I've evaluated the models using cross-validation and assess their performance based on the area under the ROC curve.

```{r}
tree_grid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  min_n(),
  levels = 2
  )

tree_grid
```

I've used the `tune_grid()` function to find the optimal combination of hyperparameters from the tuning grid.

```{r}
set.seed(as.numeric(Sys.time()))

tree_tuning <- tree_workflow %>% 
  tune_grid(
    resamples = feederwatch_forest_folds,
    grid = tree_grid
    )
```

To view the results of hyperparameter tuning, I've used the `show_best()` function. To determine the specific performance metric, I've provided the desired type of performance metric as an argument to the `show_best()` function. This allows to view the best model based on the specified performance metric.

```{r}
tree_tuning %>%
  show_best('roc_auc')
```
Based on the results, the combination of hyperparameters in grid that resulted in the highest mean ROC AUC score over 5 iterations is first one where cost_complexity is 1e-10, tree_depth is 15 and min_n is 2. This combination of hyperparameters achieved a mean ROC AUC score of 0.6079167.

It's important to note that the other combinations of hyperparameters did not perform as well in terms of ROC AUC score. The combination with cost_complexity equal 1e-10, tree_depth equal 15, and min_n equal 40 had a lower mean ROC AUC score - 0.5363150, while the combinations with tree_depth equal 1 or cost_complexity equal 0.1 resulted in a mean ROC AUC score of 0.5, indicating no predictive power.

Therefore, based on the results, the first combination of hyperparameters is the most promising choice for fitting a decision tree model to this data.

To confirm that the conclusions I've drawn are correct I've used the `select_best()` model. It's selected the model from obtained tuning results that had the best overall performance.

```{r}
best_tree <- tree_tuning %>% 
  select_best(metric = 'roc_auc')

best_tree
```

I've been right and first model was the best.


### Finalize Workflow

The final step in the hyperparameter tuning process involves utilizing the `finalize_workflow()` function to incorporate the optimal model into our workflow object.

```{r}
final_tree_workflow <- tree_workflow %>%
  finalize_workflow(best_tree)
```


## Visualize Results

I've fitted workflow to the training data. I've done it by passing workflow object to the `fit()` function. After training this model on the training data, I've proceeded to analyze variable importance using the `vip()` function. Next I've passed `tree_fit` to the `vip()` function which let to obtain ggplot object with the variable importance scores from model.

```{r}
# Fit the Model
tree_wf_fit <- final_tree_workflow %>%
  fit(data = feederwatch_forest_training)

# Exploring our Trained Model
tree_fit <- tree_wf_fit %>% 
  extract_fit_parsnip()

# Variable Importance
vip(tree_fit, aesthetics = list(fill = "lightblue", size = 0.8))
```
I've seen from the results above, that how_many with above 70% importance is the most important predictor of feederwatch. The plot suggest also that snow_dep_atleast and effort_hrs_atleast are one of the strongest predictors but their importance are about 20%. The least accurate predictor is day2_pm with less than 5%.

### Decision Tree Plot

I've visualized the trained decision tree by using the `rpart.plot()` function.

```{r}
rpart.plot(tree_fit$fit, roundint = FALSE)
```

The decision tree plot is quite large and difficult to read. There are specialized packages in R for zooming into regions of a decision tree plot as already installed `rpart.plot` or `partykit`.

```{r}
# Zoom into specific regions of the decision tree
rpart.plot(tree_fit$fit, roundint = FALSE, xlim = c(0, 0.15), ylim = c(0.01, 0.03))
```

Next I've fitted final model workflow to the training data and evaluate performance on the test data. I've reviewed performance metrics on the test data.

```{r}
tree_last_fit <- final_tree_workflow %>%
  last_fit(feederwatch_forest_split)

tree_last_fit %>%
  collect_metrics()
```
From the obtained results, the accuracy of the model is estimated to be approximately 0.9926, while the roc_auc is estimated to be around 0.6107. This indicates that:

1. The model was able to accurately predict the target variable (valid or not) for the observations in the evaluation dataset.

2. The ROC AUC score indicates that the model's predictive power is slightly better than random guessing.


```{r}
tree_predictions <- tree_last_fit %>%
  collect_predictions()

conf_mat(
  tree_predictions,
  truth = valid,
  estimate = .pred_class
  )
```
I've also decided to create confusion matrix. Based on that matrix, the model correctly predicted 7 instances as 0 so not valid and 24807 instances as 1 as valid. The model incorrectly predicted 23 instances as 1 when they were actually 0 and 163 instances as 0 when they were actually 1.

From these values, I've derived the following conclusions:

  1. The accuracy is 0.99256. The model has a high accuracy rate, indicating that it correctly classifies a large portion of the data.
  2. The precision is 0.9990737. The model has a high precision, meaning that when it predicts an instance as 1, it is usually correct.
  3. The recall is 0.9934722. The model has a high recall, indicating that it is effective at capturing most of the actual positive instances.


